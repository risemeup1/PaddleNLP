ppo:
  base:
    train_datasets: "Jsonfile::./tests/fixtures/llm/ppo_data/train.jsonl"
    eval_datasets: "Jsonfile::./tests/fixtures/llm/ppo_data/dev.jsonl"
    ptx_datasets: "Jsonfile::./tests/fixtures/llm/ppo_data/ptx.jsonl"
    max_length: 2048
    use_fusemt: 1
    use_flash_attention: 1
    max_dec_len: 1024
    min_dec_len: 1
    top_p: 0.8
    temperature: 1.0
    num_return_sequences: 1
    repetition_penalty: 1.0
    num_train_epochs: 1
    max_steps: 5
    update_iters: 1
    per_device_prompt_batch_size: 1
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 1
    learning_rate: 2e-6
    min_learning_rate: 2e-7
    weight_decay: 0.01
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.03
    recompute: 1
    recompute_granularity: "full"
    recompute_use_reentrant: 1
    critic_learning_rate: 2e-6
    critic_min_learning_rate: 2e-7
    critic_weight_decay: 0.01
    critic_lr_scheduler_type: "cosine"
    critic_warmup_ratio: 0.03
    critic_recompute: 1
    critic_recompute_granularity: "full"
    normalize_reward: 1
    normalize_advantage: 1
    kl_coeff: 0.02
    clip_range_ratio: 0.2
    clip_range_score: 10.0
    clip_range_value: 5.0
    ptx_coeff: 16.0
    logging_steps: 1
    evaluation_strategy: "no"
    per_device_eval_batch_size: 16
    eval_steps: 10000
    save_strategy: "steps"
    save_steps: 400
    save_total_limit: 5
    bf16: 1
    fp16: 0
    fp16_opt_level: O2
    do_train: 1
    do_eval: 0
    disable_tqdm: 1
    sharding_parallel_degree: 1
    sharding: stage1
    tensor_parallel_degree: 8
    tensor_parallel_output: 0
    pipeline_parallel_degree: 1
    pipeline_parallel_config: "disable_p2p_cache_shape"
    sequence_parallel: 0
    max_grad_norm: 1.0
    adam_beta1: 0.9
    adam_beta2: 0.95
    dataloader_drop_last: 0
    eval_mode: ""
    offload_level: "freeze_model optimizer train_model"
    release_grads: 1
    seed: 23
    use_fused_head_and_loss_fn: 0
    autotuner_benchmark: 0
    skip_profile_timer: 1
    fused_linear: 1

  default:
    llama:
      actor_model_name_or_path: __internal_testing__/tiny-random-llama
      reward_model_name_or_path: __internal_testing__/tiny-random-llama
